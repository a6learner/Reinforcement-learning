{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十分钟强化学习第七讲：从Q表到神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Q表的缺点：\n",
    "- 无法处理state/acton过多的情况\n",
    "- 无法处理连续值的state/action\n",
    "- 不具备泛化能力\n",
    "\n",
    "使用神经网络来解决这个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n",
    "    decay_steps = int(max_steps * decay_ratio)\n",
    "    rem_steps = max_steps - decay_steps\n",
    "    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n",
    "    values = (values - values.min()) / (values.max() - values.min())\n",
    "    values = (init_value - min_value) * values + min_value\n",
    "    values = np.pad(values, (0, rem_steps), 'edge')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,size):\n",
    "    result = np.zeros(size)\n",
    "    result[x] = 1\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2tensor(x,size):\n",
    "    x = one_hot(x,size)\n",
    "    x = torch.from_numpy(x).float()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(q_value, epsilon):\n",
    "    # detach from the graph and convert to numpy\n",
    "    # squeeze to remove the leftover dimension\n",
    "    q_value_np = q_value.clone().detach().numpy().squeeze()\n",
    "    if np.random.random() > epsilon:\n",
    "        final_move = q_value_np.argmax()\n",
    "    else:\n",
    "        final_move = np.random.randint(len(q_value_np))\n",
    "    return final_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单版本的DQN\n",
    "def Simple_DQN(env,lr = 0.001,episodes=100, max_step = 100,gamma=0.9,test_policy_freq=100):\n",
    "\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    epsilons = decay_schedule(1,0.01,0.8, episodes)\n",
    "\n",
    "    model = Linear(nS, nA)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(episodes): \n",
    "        state, _ = env.reset()\n",
    "        state = conv2tensor(state,nS)\n",
    "        finished = False\n",
    "        step = 0\n",
    "        while not finished :\n",
    "            q_value = model(state)\n",
    "\n",
    "            # take action\n",
    "            action = select_action(q_value,epsilons[i])\n",
    "            next_state, reward, finished, _, _ = env.step(action)\n",
    "            next_state = conv2tensor(next_state,nS)\n",
    "\n",
    "            # find target\n",
    "            target = q_value.clone().detach()\n",
    "            q_value_next = model(next_state).detach().numpy().squeeze()\n",
    "            td_target = reward + gamma * q_value_next.max() * (not finished)\n",
    "            target[action] = td_target\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            td_error = loss_fn(q_value,target)\n",
    "            td_error.backward()\n",
    "            optimizer.step()\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "            if step >= max_step:\n",
    "                break\n",
    "\n",
    "        if finished:\n",
    "            results.append(reward)\n",
    "\n",
    "        \n",
    "        if (i>0) and (i % test_policy_freq == 0):\n",
    "            results_array = np.array(results)\n",
    "            print(\"Running episode  {} Reaches goal {:.2f}%. \".format(\n",
    "                i, \n",
    "                results_array[-100:].mean()*100))\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\machine_learning_study\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode  1000 Reaches goal 4.00%. \n",
      "Running episode  2000 Reaches goal 2.00%. \n",
      "Running episode  3000 Reaches goal 7.00%. \n",
      "Running episode  4000 Reaches goal 6.00%. \n",
      "Running episode  5000 Reaches goal 15.00%. \n",
      "Running episode  6000 Reaches goal 15.00%. \n",
      "Running episode  7000 Reaches goal 13.00%. \n",
      "Running episode  8000 Reaches goal 27.00%. \n",
      "Running episode  9000 Reaches goal 32.00%. \n",
      "Running episode  10000 Reaches goal 30.00%. \n",
      "Running episode  11000 Reaches goal 36.00%. \n",
      "Running episode  12000 Reaches goal 26.00%. \n",
      "Running episode  13000 Reaches goal 45.00%. \n",
      "Running episode  14000 Reaches goal 61.00%. \n",
      "Running episode  15000 Reaches goal 62.00%. \n",
      "Running episode  16000 Reaches goal 49.00%. \n",
      "Running episode  17000 Reaches goal 54.00%. \n",
      "Running episode  18000 Reaches goal 64.00%. \n",
      "Running episode  19000 Reaches goal 58.00%. \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "Simple_DQN(env,lr = 0.001,episodes=20000, max_step = 100,gamma=0.9,test_policy_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode  1000 Reaches goal 0.00%. \n",
      "Running episode  2000 Reaches goal 0.00%. \n",
      "Running episode  3000 Reaches goal 1.00%. \n",
      "Running episode  4000 Reaches goal 2.00%. \n",
      "Running episode  5000 Reaches goal 1.00%. \n",
      "Running episode  6000 Reaches goal 2.00%. \n",
      "Running episode  7000 Reaches goal 3.00%. \n",
      "Running episode  8000 Reaches goal 3.00%. \n",
      "Running episode  9000 Reaches goal 9.00%. \n",
      "Running episode  10000 Reaches goal 4.00%. \n",
      "Running episode  11000 Reaches goal 4.00%. \n",
      "Running episode  12000 Reaches goal 7.00%. \n",
      "Running episode  13000 Reaches goal 1.00%. \n",
      "Running episode  14000 Reaches goal 18.00%. \n",
      "Running episode  15000 Reaches goal 2.00%. \n",
      "Running episode  16000 Reaches goal 17.00%. \n",
      "Running episode  17000 Reaches goal 0.00%. \n",
      "Running episode  18000 Reaches goal 12.00%. \n",
      "Running episode  19000 Reaches goal 32.00%. \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1',map_name=\"8x8\")\n",
    "Simple_DQN(env,lr = 0.001,episodes=20000, max_step = 100,gamma=0.9,test_policy_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 面临的问题\n",
    "- 用梯度下降算法来实现有一定条件，1）target是稳定的  2）样本是iid的\n",
    "- 但在强化学习中不满足以上两个条件\n",
    "  - Non-stationary target ：target不稳定\n",
    "    - td_target = reward + gamma * q_value_next.max() * (not finished)\n",
    "  - No independent and identically distributed ：上一个样本和下一个样本在一个连续的游戏中\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
