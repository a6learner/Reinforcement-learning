{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十分钟强化学习第九讲：DQN的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN\n",
    "- Q-Learning是用max来估计Q值的，估计Q值偏高过于乐观。DQN也有同样的问题\n",
    "- 一种解决思路是使用两个网络，一个用来选择max的Q值对应的action，另一个来负责输出对应的Q值，二者合作更为谨慎。\n",
    "- DQN已经有了一个target Network，所以直接用它了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN\n",
    "- 可以将Q进行拆解： Q = V + A，将状态和行动的收益分开来建模。\n",
    "- 分别用神经网络来近似，更好的学习到Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prioritized replay buffer\n",
    "- 过去的经验中有重要的有不重要\n",
    "- 对重要的经验进行加权抽样，反复进行学习\n",
    "- 用abs(error)当做经验的重要性度量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
