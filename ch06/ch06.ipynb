{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十分钟强化学习第六讲：Model-Based方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 之前的三种方法，我们不知道环境内在机制，也就是转换函数  (model-free)\n",
    "- 但是agent能否在交互时同时学习估计这种函数，并利用这种函数  (model-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from help import FrozenLake, print_policy, test_game\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n",
    "    decay_steps = int(max_steps * decay_ratio)\n",
    "    rem_steps = max_steps - decay_steps\n",
    "    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n",
    "    values = (values - values.min()) / (values.max() - values.min())\n",
    "    values = (init_value - min_value) * values + min_value\n",
    "    values = np.pad(values, (0, rem_steps), 'edge')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, Q, epsilon):\n",
    "    if np.random.random() > epsilon:\n",
    "        return np.argmax(Q[state])\n",
    "    else:\n",
    "        return np.random.randint(len(Q[state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyna_q(env,episodes=100,gamma=0.9,n_planning = 3,test_policy_freq=1000):\n",
    "    nS, nA = 16,4\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    alphas = decay_schedule(0.5,0.01,0.5, episodes)\n",
    "    epsilons = decay_schedule(1,0.01,0.8, episodes)\n",
    "\n",
    "    T_count = np.zeros((nS, nA, nS), dtype=np.int32)\n",
    "    R_model = np.zeros((nS, nA, nS), dtype=np.float64)\n",
    "    planning_track = []\n",
    "\n",
    "    for i in range(episodes): \n",
    "        state = env.reset()\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            action = select_action(state, Q, epsilons[i])\n",
    "            next_state, reward, finished = env.step(action)\n",
    "\n",
    "            #  记录环境反馈信息，统计转移次数和回报\n",
    "            T_count[state][action][next_state] += 1\n",
    "            r_diff = reward - R_model[state][action][next_state]\n",
    "            R_model[state][action][next_state] += (r_diff / T_count[state][action][next_state])\n",
    "\n",
    "            target = reward + gamma * Q[next_state].max() * (not finished)\n",
    "            error = target - Q[state][action]\n",
    "            Q[state][action] = Q[state][action] + alphas[i] * error\n",
    "\n",
    "            backup_next_state = next_state\n",
    "            # 进入规划循环\n",
    "            for _ in range(n_planning):\n",
    "                if Q.sum() == 0: break\n",
    "                # 选择一个曾经进入过的状态\n",
    "                visited_states = np.where(np.sum(T_count, axis=(1, 2)) > 0)[0]\n",
    "                state = np.random.choice(visited_states)\n",
    "                # 选择一个曾经选择过的行动\n",
    "                actions_taken = np.where(np.sum(T_count[state], axis=1) > 0)[0]\n",
    "                action = np.random.choice(actions_taken)\n",
    "                # 根据环境模型计算出可能的下一步状态和可能的回报\n",
    "                probs = T_count[state][action]/T_count[state][action].sum()\n",
    "                next_state = np.random.choice(np.arange(nS), size=1, p=probs)[0]\n",
    "                reward = R_model[state][action][next_state]\n",
    "                planning_track.append((state, action, reward, next_state))\n",
    "\n",
    "                target = reward + gamma * Q[next_state].max()\n",
    "                error = target - Q[state][action]\n",
    "                Q[state][action] = Q[state][action] + alphas[i] * error\n",
    "            \n",
    "            state = backup_next_state\n",
    "\n",
    "\n",
    "        pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "        \n",
    "        if i % test_policy_freq == 0:\n",
    "                print(\"Test episode {} Reaches goal {:.2f}%. \".format\n",
    "                (i, test_game(env, pi)*100))\n",
    "\n",
    "    return pi,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode 0 Reaches goal 0.00%. \n",
      "Test episode 1000 Reaches goal 41.00%. \n",
      "Test episode 2000 Reaches goal 36.00%. \n",
      "Test episode 3000 Reaches goal 28.00%. \n",
      "Test episode 4000 Reaches goal 68.00%. \n",
      "Test episode 5000 Reaches goal 53.00%. \n",
      "Test episode 6000 Reaches goal 63.00%. \n",
      "Test episode 7000 Reaches goal 25.00%. \n",
      "Test episode 8000 Reaches goal 61.00%. \n",
      "Test episode 9000 Reaches goal 70.00%. \n",
      "Test episode 10000 Reaches goal 65.00%. \n",
      "Test episode 11000 Reaches goal 64.00%. \n",
      "Test episode 12000 Reaches goal 43.00%. \n",
      "Test episode 13000 Reaches goal 37.00%. \n",
      "Test episode 14000 Reaches goal 78.00%. \n",
      "Test episode 15000 Reaches goal 62.00%. \n",
      "Test episode 16000 Reaches goal 72.00%. \n",
      "Test episode 17000 Reaches goal 70.00%. \n",
      "Test episode 18000 Reaches goal 33.00%. \n",
      "Test episode 19000 Reaches goal 72.00%. \n"
     ]
    }
   ],
   "source": [
    "policy_qlearning,Q_qlearning = dyna_q(env,episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_sampling(env,episodes=100,gamma=0.9,\n",
    "                        planning_freq = 5,\n",
    "                        max_trajectory_depth=100,\n",
    "                        test_policy_freq=1000):\n",
    "    nS, nA = 16,4\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    alphas = decay_schedule(0.5,0.01,0.5, episodes)\n",
    "    epsilons = decay_schedule(1,0.01,0.8, episodes)\n",
    "\n",
    "    T_count = np.zeros((nS, nA, nS), dtype=np.int32)\n",
    "    R_model = np.zeros((nS, nA, nS), dtype=np.float64)\n",
    "    planning_track = []\n",
    "\n",
    "    for i in range(episodes): \n",
    "        state = env.reset()\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            action = select_action(state, Q, epsilons[i])\n",
    "            next_state, reward, finished = env.step(action)\n",
    "\n",
    "            #  记录环境反馈信息，统计转移次数和回报\n",
    "            T_count[state][action][next_state] += 1\n",
    "            r_diff = reward - R_model[state][action][next_state]\n",
    "            R_model[state][action][next_state] += (r_diff / T_count[state][action][next_state])\n",
    "\n",
    "            target = reward + gamma * Q[next_state].max() * (not finished)\n",
    "            error = target - Q[state][action]\n",
    "            Q[state][action] = Q[state][action] + alphas[i] * error\n",
    "\n",
    "            backup_next_state = next_state\n",
    "            # 进入规划循环\n",
    "            if i % planning_freq == 0:\n",
    "                for _ in range(max_trajectory_depth):\n",
    "                    if Q.sum() == 0: break\n",
    "                    # 从当前实际的状态进行规划\n",
    "                    action = Q[state].argmax()\n",
    "                    if not T_count[state][action].sum(): break\n",
    "                    probs = T_count[state][action]/T_count[state][action].sum()\n",
    "                    next_state = np.random.choice(np.arange(nS), size=1, p=probs)[0]\n",
    "                    reward = R_model[state][action][next_state]\n",
    "                    planning_track.append((state, action, reward, next_state))\n",
    "\n",
    "                    target = reward + gamma * Q[next_state].max()\n",
    "                    error = target - Q[state][action]\n",
    "                    Q[state][action] = Q[state][action] + alphas[i] * error\n",
    "\n",
    "                    state = next_state\n",
    "            \n",
    "            state = backup_next_state\n",
    "\n",
    "\n",
    "        pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "        \n",
    "        if i % test_policy_freq == 0:\n",
    "                print(\"Test episode {} Reaches goal {:.2f}%. \".format\n",
    "                (i, test_game(env, pi)*100))\n",
    "\n",
    "    return pi,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode 0 Reaches goal 0.00%. \n",
      "Test episode 1000 Reaches goal 60.00%. \n",
      "Test episode 2000 Reaches goal 37.00%. \n",
      "Test episode 3000 Reaches goal 63.00%. \n",
      "Test episode 4000 Reaches goal 37.00%. \n",
      "Test episode 5000 Reaches goal 72.00%. \n",
      "Test episode 6000 Reaches goal 55.00%. \n",
      "Test episode 7000 Reaches goal 52.00%. \n",
      "Test episode 8000 Reaches goal 76.00%. \n",
      "Test episode 9000 Reaches goal 72.00%. \n",
      "Test episode 10000 Reaches goal 77.00%. \n",
      "Test episode 11000 Reaches goal 49.00%. \n",
      "Test episode 12000 Reaches goal 69.00%. \n",
      "Test episode 13000 Reaches goal 77.00%. \n",
      "Test episode 14000 Reaches goal 70.00%. \n",
      "Test episode 15000 Reaches goal 72.00%. \n",
      "Test episode 16000 Reaches goal 50.00%. \n",
      "Test episode 17000 Reaches goal 55.00%. \n",
      "Test episode 18000 Reaches goal 67.00%. \n",
      "Test episode 19000 Reaches goal 77.00%. \n"
     ]
    }
   ],
   "source": [
    "policy_qlearning,Q_qlearning = trajectory_sampling(env,episodes=20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
