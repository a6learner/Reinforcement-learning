{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十分钟强化学习第四讲：蒙特卡罗方法\n",
    "在Policy Iteration中我们知道状态转移函数的所有细节，  \n",
    "但实际情况下，我们只能进行与环境的交互，不知道状态转移的具体细节  \n",
    "蒙特卡罗方法实际上就是不断模拟来获得一个统计值来近似真实值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from help import FrozenLake, print_policy, test_game\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, Q, mode=\"both\"):\n",
    "    if mode == \"explore\":\n",
    "        return np.random.randint(len(Q[state]))\n",
    "    if mode == \"exploit\":\n",
    "        return np.argmax(Q[state])\n",
    "    if mode == \"both\":\n",
    "        if np.random.random() > 0.5:\n",
    "            return np.argmax(Q[state])\n",
    "        else:\n",
    "            return np.random.randint(len(Q[state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q ,max_steps=200):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    step = 0\n",
    "\n",
    "    while not finished:\n",
    "        action = select_action(state, Q, mode='both')\n",
    "        next_state, reward, finished = env.step(action)\n",
    "        experience = (state, action, finished,reward)\n",
    "        episode.append(experience)\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "    return np.array(episode,dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何算Q：计算单条轨迹下的平均return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env, episodes=10000, test_policy_freq=1000):\n",
    "    nS, nA = 16, 4\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    returns = {} \n",
    "\n",
    "    for i in range(episodes): \n",
    "        episode = play_game(env, Q)\n",
    "        visited = np.zeros((nS, nA), dtype=bool)\n",
    "\n",
    "        for t, (state, action, _, _) in enumerate(episode):\n",
    "            state_action = (state, action)\n",
    "            # 状态行为对只取第一次出现的\n",
    "            if not visited[state][action]:\n",
    "                visited[state][action] = True\n",
    "                discount = np.array([0.9**i for i in range(len(episode[t:]))])\n",
    "                reward = episode[t:, -1]\n",
    "                G = np.sum( discount * reward)\n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]  \n",
    "\n",
    "                Q[state][action] = sum(returns[state_action]) / len(returns[state_action])\n",
    "                #Q[state][action] = Q[state][action] + 1/len(returns[state_action]) * (G - Q[state][action])\n",
    "        pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "\n",
    "        if i % test_policy_freq == 0:\n",
    "                print(\"Test episode {} Reaches goal {:.2f}%. \".format\n",
    "                (i, test_game(env, pi)*100))\n",
    "            \n",
    "    return pi,Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增量平均的数学推导**\n",
    "\n",
    "1. **样本均值的基本公式**\n",
    "\n",
    "   给定一个数据序列：$x_1, x_2, \\dots, x_n$，其算术平均值为：\n",
    "\n",
    "   $$\n",
    "   \\bar{x}_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "   $$\n",
    "\n",
    "2. **增量平均的递推公式推导**\n",
    "\n",
    "   当新数据点 $x_{n+1}$ 到来时，新的均值为：\n",
    "\n",
    "   $$\n",
    "   \\bar{x}_{n+1} = \\frac{1}{n+1} \\sum_{i=1}^{n+1} x_i\n",
    "   $$\n",
    "\n",
    "   将求和部分拆分：\n",
    "\n",
    "   $$\n",
    "   \\bar{x}_{n+1} = \\frac{1}{n+1} \\left( \\sum_{i=1}^{n} x_i + x_{n+1} \\right)\n",
    "   $$\n",
    "\n",
    "   由于 $\\sum_{i=1}^{n} x_i = n \\cdot \\bar{x}_n$，代入后得到：\n",
    "\n",
    "   $$\n",
    "   \\bar{x}_{n+1} = \\frac{1}{n+1} \\left( n \\cdot \\bar{x}_n + x_{n+1} \\right)\n",
    "   $$\n",
    "\n",
    "   展开并整理：\n",
    "\n",
    "   $$\n",
    "   \\bar{x}_{n+1} = \\frac{n}{n+1} \\bar{x}_n + \\frac{1}{n+1} x_{n+1}\n",
    "   $$\n",
    "\n",
    "   进一步表示为增量形式：\n",
    "\n",
    "   $$\n",
    "   \\bar{x}_{n+1} = \\bar{x}_n + \\frac{1}{n+1} \\left( x_{n+1} - \\bar{x}_n \\right)\n",
    "   $$\n",
    "\n",
    "   其中，$\\frac{1}{n+1}$ 是学习率，随着样本数量的增加而减小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode 0 Reaches goal 0.00%. \n",
      "Test episode 1000 Reaches goal 40.00%. \n",
      "Test episode 2000 Reaches goal 32.00%. \n",
      "Test episode 3000 Reaches goal 49.00%. \n",
      "Test episode 4000 Reaches goal 37.00%. \n",
      "Test episode 5000 Reaches goal 38.00%. \n",
      "Test episode 6000 Reaches goal 53.00%. \n",
      "Test episode 7000 Reaches goal 46.00%. \n",
      "Test episode 8000 Reaches goal 42.00%. \n",
      "Test episode 9000 Reaches goal 37.00%. \n",
      "Test episode 10000 Reaches goal 48.00%. \n",
      "Test episode 11000 Reaches goal 56.00%. \n",
      "Test episode 12000 Reaches goal 61.00%. \n",
      "Test episode 13000 Reaches goal 49.00%. \n",
      "Test episode 14000 Reaches goal 53.00%. \n",
      "Test episode 15000 Reaches goal 47.00%. \n",
      "Test episode 16000 Reaches goal 43.00%. \n",
      "Test episode 17000 Reaches goal 44.00%. \n",
      "Test episode 18000 Reaches goal 39.00%. \n",
      "Test episode 19000 Reaches goal 46.00%. \n"
     ]
    }
   ],
   "source": [
    "policy_mc,Q = monte_carlo(env,episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "| 00      v | 01      ^ | 02      < | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      > |           |\n",
      "Reaches goal 49.00%. \n"
     ]
    }
   ],
   "source": [
    "print_policy(policy_mc)\n",
    "print('Reaches goal {:.2f}%. '.format(test_game(env, policy_mc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蒙特卡罗方法的缺点：\n",
    "- 要等到游戏一轮完结后才更新\n",
    "- 利用的信息中噪声较多，学习效率较低"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
