{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十分钟强化学习第四讲：蒙特卡罗方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from help import FrozenLake, print_policy, test_game\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, Q, mode=\"both\"):\n",
    "    if mode == \"explore\":\n",
    "        return np.random.randint(len(Q[state]))\n",
    "    if mode == \"exploit\":\n",
    "        return np.argmax(Q[state])\n",
    "    if mode == \"both\":\n",
    "        if np.random.random() > 0.5:\n",
    "            return np.argmax(Q[state])\n",
    "        else:\n",
    "            return np.random.randint(len(Q[state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q ,max_steps=200):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    step = 0\n",
    "\n",
    "    while not finished:\n",
    "        action = select_action(state, Q, mode='both')\n",
    "        next_state, reward, finished = env.step(action)\n",
    "        experience = (state, action, finished,reward)\n",
    "        episode.append(experience)\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "    return np.array(episode,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env, episodes=10000, test_policy_freq=1000):\n",
    "    nS, nA = 16, 4\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    returns = {} \n",
    "\n",
    "    for i in range(episodes): \n",
    "        episode = play_game(env, Q)\n",
    "        visited = np.zeros((nS, nA), dtype=bool)\n",
    "\n",
    "        for t, (state, action, _, _) in enumerate(episode):\n",
    "            state_action = (state, action)\n",
    "            if not visited[state][action]:\n",
    "                visited[state][action] = True\n",
    "                discount = np.array([0.9**i for i in range(len(episode[t:]))])\n",
    "                reward = episode[t:, -1]\n",
    "                G = np.sum( discount * reward)\n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]  \n",
    "\n",
    "                Q[state][action] = sum(returns[state_action]) / len(returns[state_action])\n",
    "                #Q[state][action] = Q[state][action] + 1/len(returns[state_action]) * (G - Q[state][action])\n",
    "        pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "\n",
    "        if i % test_policy_freq == 0:\n",
    "                print(\"Test episode {} Reaches goal {:.2f}%. \".format\n",
    "                (i, test_game(env, pi)*100))\n",
    "            \n",
    "    return pi,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode 0 Reaches goal 0.00%. \n",
      "Test episode 1000 Reaches goal 7.00%. \n",
      "Test episode 2000 Reaches goal 13.00%. \n",
      "Test episode 3000 Reaches goal 31.00%. \n",
      "Test episode 4000 Reaches goal 48.00%. \n",
      "Test episode 5000 Reaches goal 38.00%. \n",
      "Test episode 6000 Reaches goal 43.00%. \n",
      "Test episode 7000 Reaches goal 66.00%. \n",
      "Test episode 8000 Reaches goal 71.00%. \n",
      "Test episode 9000 Reaches goal 78.00%. \n",
      "Test episode 10000 Reaches goal 70.00%. \n",
      "Test episode 11000 Reaches goal 76.00%. \n",
      "Test episode 12000 Reaches goal 78.00%. \n",
      "Test episode 13000 Reaches goal 79.00%. \n",
      "Test episode 14000 Reaches goal 68.00%. \n",
      "Test episode 15000 Reaches goal 71.00%. \n",
      "Test episode 16000 Reaches goal 70.00%. \n",
      "Test episode 17000 Reaches goal 68.00%. \n",
      "Test episode 18000 Reaches goal 68.00%. \n",
      "Test episode 19000 Reaches goal 71.00%. \n"
     ]
    }
   ],
   "source": [
    "policy_mc,Q = monte_carlo(env,episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "| 00      > | 01      ^ | 02      < | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 42.00%. \n"
     ]
    }
   ],
   "source": [
    "print_policy(policy_mc)\n",
    "print('Reaches goal {:.2f}%. '.format(test_game(env, policy_mc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蒙特卡罗方法的缺点：\n",
    "- 要等到游戏一轮完结后才更新\n",
    "- 利用的信息中噪声较多，学习效率较低"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
